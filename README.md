# ğŸ“ CollegeAdvisor Data Pipeline & Fine-Tuning

A production-ready data pipeline and fine-tuning system for college admissions AI assistance.

---

## ğŸš€ Quick Start - Fine-Tuning

### One-Command Fine-Tuning

```bash
# 1. Activate virtual environment
source venv_finetune/bin/activate

# 2. Run fine-tuning
./run_finetuning.sh
```

That's it! The script will:
- âœ… Validate your system
- âœ… Download training data from R2
- âœ… Process and validate data
- âœ… Train the model
- âœ… Save checkpoints and final model

**See:** [`UNIFIED_FINETUNING_GUIDE.md`](UNIFIED_FINETUNING_GUIDE.md) for complete documentation.

---

## ğŸ“‹ Table of Contents

- [Fine-Tuning](#fine-tuning)
- [Data Pipeline](#data-pipeline)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Configuration](#configuration)
- [Documentation](#documentation)
- [Support](#support)

---

## ğŸ¯ Fine-Tuning

### Unified Fine-Tuning System

We've consolidated **14 different scripts** into a **single, production-ready solution**:

**Main Script:** `unified_finetune.py`
**Launcher:** `run_finetuning.sh`
**Documentation:** `UNIFIED_FINETUNING_GUIDE.md`

### Features

- âœ… **Automatic R2 Data Fetching** - Downloads training data with integrity verification
- âœ… **Comprehensive Validation** - Pre-flight checks for dependencies, disk space, memory
- âœ… **MacBook Optimized** - Works on Apple Silicon (MPS) and Intel (CPU)
- âœ… **Robust Error Handling** - Extensive error checking with clear messages
- âœ… **Memory Efficient** - Optimized for MacBook hardware constraints
- âœ… **Checkpoint Support** - Automatic saving with resume capability
- âœ… **Real-time Monitoring** - Progress tracking with detailed logging

### Quick Commands

```bash
# Run fine-tuning
./run_finetuning.sh

# Or run directly
python unified_finetune.py
```

### Documentation

- **[Unified Fine-Tuning Guide](UNIFIED_FINETUNING_GUIDE.md)** - Complete usage guide
- **[Migration Guide](MIGRATION_TO_UNIFIED_FINETUNING.md)** - Migrating from old scripts
- **[Consolidation Summary](FINETUNING_CONSOLIDATION_SUMMARY.md)** - Technical details

---

## ğŸ“Š Data Pipeline

### Overview

Production-ready data collection and processing pipeline with:
- Multiple data source collectors
- Real-time quality monitoring
- Automated pipelines
- R2 cloud storage integration

### Data Sources

1. **Government APIs**
   - College Scorecard API
   - IPEDS Data
   - State Education APIs

2. **Institutional Data**
   - Carnegie Classification
   - University websites
   - Financial aid databases

3. **Social Media**
   - Reddit discussions
   - Twitter mentions
   - YouTube content

4. **User Data** (Production)
   - Authentication events
   - User profiles
   - Interaction data

### Data Collection

```bash
# Collect from all sources
python -m college_advisor_data.cli collect-all

# Collect from specific source
python -m college_advisor_data.cli collect --source scorecard
```

### Data Quality Monitoring

```bash
# Run quality checks
python monitoring/data_quality_monitor.py

# View quality reports
ls data/quality_reports/
```

---

## ğŸ“ Project Structure

```
CollegeAdvisor-data/
â”œâ”€â”€ unified_finetune.py              # ğŸ¯ Main fine-tuning script
â”œâ”€â”€ run_finetuning.sh                # ğŸš€ Fine-tuning launcher
â”œâ”€â”€ UNIFIED_FINETUNING_GUIDE.md      # ğŸ“– Complete guide
â”‚
â”œâ”€â”€ college_advisor_data/            # Core data pipeline
â”‚   â”œâ”€â”€ ingestion/                   # Data ingestion
â”‚   â”œâ”€â”€ preprocessing/               # Data processing
â”‚   â”œâ”€â”€ embedding/                   # Vector embeddings
â”‚   â”œâ”€â”€ storage/                     # R2 storage client
â”‚   â””â”€â”€ evaluation/                  # Quality evaluation
â”‚
â”œâ”€â”€ collectors/                      # Data collectors
â”‚   â”œâ”€â”€ government.py               # Government APIs
â”‚   â”œâ”€â”€ web_scrapers.py             # Web scraping
â”‚   â”œâ”€â”€ social_media.py             # Social media
â”‚   â””â”€â”€ financial_aid.py            # Financial aid data
â”‚
â”œâ”€â”€ ai_training/                     # AI training modules
â”‚   â”œâ”€â”€ finetuning_data_prep.py     # Data preparation
â”‚   â”œâ”€â”€ training_pipeline.py        # Training pipeline
â”‚   â”œâ”€â”€ continuous_learning.py      # Continuous learning
â”‚   â””â”€â”€ data_quality.py             # Quality monitoring
â”‚
â”œâ”€â”€ monitoring/                      # Monitoring & alerts
â”‚   â”œâ”€â”€ data_quality_monitor.py     # Quality monitoring
â”‚   â””â”€â”€ pipeline_health_monitor.py  # Pipeline health
â”‚
â”œâ”€â”€ configs/                         # Configuration files
â”‚   â”œâ”€â”€ api_config.yaml             # API endpoints
â”‚   â””â”€â”€ database_config.yaml        # Database config
â”‚
â”œâ”€â”€ data/                           # Data storage
â”‚   â”œâ”€â”€ raw/                        # Raw data
â”‚   â”œâ”€â”€ processed/                  # Processed data
â”‚   â”œâ”€â”€ training/                   # Training datasets
â”‚   â””â”€â”€ quality_reports/            # Quality reports
â”‚
â””â”€â”€ logs/                           # Logs
    â”œâ”€â”€ finetuning/                 # Fine-tuning logs
    â””â”€â”€ pipeline.log                # Pipeline logs
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.8+
- 8GB+ RAM (16GB recommended)
- 10GB+ free disk space

### Setup

```bash
# 1. Clone repository
git clone <repository-url>
cd CollegeAdvisor-data

# 2. Create virtual environment
python3 -m venv venv_finetune
source venv_finetune/bin/activate

# 3. Install dependencies
pip install -r requirements-finetuning.txt

# 4. Configure environment
cp .env.example .env
# Edit .env with your credentials
```

### R2 Configuration

Ensure your `.env` file contains:

```bash
R2_ACCOUNT_ID=your_account_id
R2_ACCESS_KEY_ID=your_access_key
R2_SECRET_ACCESS_KEY=your_secret_key
R2_BUCKET_NAME=collegeadvisor-finetuning-data
COLLEGE_SCORECARD_API_KEY=your_api_key
```

---

## âš™ï¸ Configuration

### Fine-Tuning Configuration

Edit `unified_finetune.py` - `FineTuningConfig` class:

```python
@dataclass
class FineTuningConfig:
    # Model
    model_name: str = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    max_seq_length: int = 1024

    # LoRA
    lora_r: int = 32
    lora_alpha: int = 64

    # Training
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 2
    learning_rate: float = 2e-5
```

### Pipeline Configuration

- **API Config:** `configs/api_config.yaml`
- **Database Config:** `configs/database_config.yaml`
- **Quality Monitoring:** `data/quality_monitoring_config.yaml`

---

## ğŸ“š Documentation

### Fine-Tuning

- **[Unified Fine-Tuning Guide](UNIFIED_FINETUNING_GUIDE.md)** - Complete usage guide
- **[Migration Guide](MIGRATION_TO_UNIFIED_FINETUNING.md)** - Migrating from old scripts
- **[Consolidation Summary](FINETUNING_CONSOLIDATION_SUMMARY.md)** - Technical overview

### Data Pipeline

- **[Quick Start](QUICK_START.md)** - Getting started
- **[Production Deployment](PRODUCTION_DEPLOYMENT_GUIDE.md)** - Production setup
- **[Data Expansion Strategy](DATA_EXPANSION_STRATEGY.md)** - Expanding data sources

### API & Integration

- **[API Integration](API_INTEGRATION_INSTRUCTIONS.md)** - API setup
- **[R2 Setup](R2_SETUP_COMPLETE.md)** - R2 configuration

---

## ğŸ†˜ Support

### Troubleshooting

1. **Check logs:** `logs/finetuning/unified_finetune_*.log`
2. **Review guides:** See documentation links above
3. **Verify setup:** Run system validation in the script

### Common Issues

**Issue: R2 credentials not found**
```bash
# Check .env file
cat .env | grep R2_
```

**Issue: Out of memory**
```python
# Reduce batch size in FineTuningConfig
per_device_train_batch_size = 1
gradient_accumulation_steps = 16
```

**Issue: Dependencies missing**
```bash
# Reinstall dependencies
pip install -r requirements-finetuning.txt
```

---

## ğŸ¯ Next Steps

1. **Run Fine-Tuning**
   ```bash
   ./run_finetuning.sh
   ```

2. **Test Your Model**
   - See testing section in `UNIFIED_FINETUNING_GUIDE.md`

3. **Deploy to Production**
   - See `PRODUCTION_DEPLOYMENT_GUIDE.md`

4. **Expand Data Sources**
   - See `DATA_EXPANSION_STRATEGY.md`

---

## ğŸ“Š Status

- âœ… **Fine-Tuning:** Production Ready
- âœ… **Data Pipeline:** Production Ready
- âœ… **R2 Integration:** Complete
- âœ… **Quality Monitoring:** Active
- âœ… **Documentation:** Complete

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ™ Acknowledgments

Built with:
- HuggingFace Transformers
- PyTorch
- PEFT (Parameter-Efficient Fine-Tuning)
- Cloudflare R2
- ChromaDB

---

**Ready to fine-tune? Run:** `./run_finetuning.sh`
